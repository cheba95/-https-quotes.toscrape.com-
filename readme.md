# Парсер для сбора данных с сайта <https://quotes.toscrape.com/>

## a. Что было сделано

Согласно заданию необходимо произвести сбор данных с сайта <https://quotes.toscrape.com/>.

Поскольку не уточнено, какие именно данные необходимо собрать, предположил, что необходимо собрать все возможные данные и структурировать их.

Указанный сайт содержит цитаты с указанием авторов и тэгов, тэги со ссылками на соответствующие им цитаты, топ-10 тэгов,сведения об авторах цитат.

Соответственно, поставил себе задачу собрать всю указанную информацию и сгруппировать её.

Поскольку в задании указано выгрузить сведения в один JSON-файл, я подготовил скрипт (**parser_main.py**), собирающий и выгружающий данные с сайта в файл **quotes_main.json** в следующем порядке:

1. Список со словарями, каждый из которых содержит цитату, её автора и тэги.
2. Словарь с топом тэгов.
3. Словарь с тэгами и ссылками на них.
4. Словарь с авторами и сведениями о них.

Данный скрипт сохраняет данные в JSON-файл с ключом _ensure_ascii == True (значение по умолчанию)_, то есть с перекодировкой не-ascii символов. Это хуже с точки зрения читабельности, но лучше с точки зрения возможной последующей обработки другими программами.

При сохранении в JSON-файл словарь с тэгами и словарь с авторами и сведениями сортируется для удобства работы. Топ тэгов уже отсортирован по популярности изначально, список со словарями с цитатами сортировать нет необходимости.

Кроме того, дополнительно я решил написать расширенный скрипт (**parser_plus.py**), который в дополнение к основному скрипту делает также следующее:

1. Создает отдельные JSON-файлы (в дополнение к одному большому файлу со всей этой информацией):
   - с цитатами (_quotes_only.json_ и _quotes_only_human_read_);
   - с топом тэгов (_top_tags.json_);
   - с тэгами и ссылками на них (_tags_dictionary.json_);
   - с авторами и сведениями о них (_authors_info.json_ и _authors_info_human_read.json_).
2. Создает словарь с авторами в качестве ключей и их цитатами в качестве значений - для быстрого поиска цитат по автору. Выгружается в отдельный файл (_quotes_by_author.json_ и _quotes_by_author_human_read.json_).
3. Создает словарь с тэгами в качестве ключей и соответствующими цитатами и авторами в качестве значений - для быстрого поиска цитат по тэгам. Выгружается в отдельный файл (_quotes_by_tag.json_ и _quotes_by_tag_human_read_).
4. Добавляет к словарю с авторами и сведениями о них цитаты соответствующих авторов. По автору как ключу в этом словаре можно найти сведения о нём + все его цитаты. Выгружается в отдельный файл (_authors_info_with_quotes.json_ и _authors_info_with_quotes_human_read.json_)

Те файлы, которые содержат не-ascii символы, сохраняются в двух вариантах - с кодировкой и без (_ensure_ascii == False_). Вариант с кодировкой предпочтительнее для машинной обработки, вариант без кодировки удобнее для чтения человеком. Файлы без кодировки имеют в конце название приписку \__human_read_.

Указанный расширенный скрипт сохраняет информацию в различных вариантах, чтобы в дальнейшем, в зависимости от задач, можно было использовать более подходящие по содержанию JSON-файлы.

**указать в какие папки и как сохранены JSON-файлы и скрипты.**

## b. Откуда были получены данные

Данные получены с сайта <https://quotes.toscrape.com/>, в т.ч. с его главной страницы, с последующих страниц с цитатами (,<https://quotes.toscrape.com/page/2/> и последующими вплоть до <https://quotes.toscrape.com/page/10/>), а также со страниц с биографиями авторов цитат (например, <https://quotes.toscrape.com/author/Albert-Einstein/> и подобным ей).

## c. Как осуществлялся сбор

Сбор осуществлялся с помощью модулей _requests_ и _json_ и библиотеки _BeautifulSoup_. С помощью модуля _requests_ направляется запрос на веб-страницу. Возвращенный веб-страницей HTML-документ передается в конструктор (функцию) _BeautifulSoup()_, который с помощью парсера _html.parser_ сохраняет в переменную _page_ объект BeautifulSoup. Данный объект содержит сведения с веб-страницы с возможностью поиска методами _.find_ и _.find_all_.
С помощью этих методов и классов элементов находим интересующую нас информацию.

Нужные классы мы можем найти, открыв страницу, например, браузером Google Chrome и сочетанием клавиш _Ctrl-Shift-I_ открыть DevTools (инструменты разработчика) и во вкладке _Elements_ найти интересующие элементы и их тэги и классы.

Разберем скрипт на примере обработки цитат. В элементах с тэгом _div_ и классом _quote_ содержатся блоки с цитатами. В свою очередь, в этих элементах находятся следующие элементы:

- текст цитаты - элемент с тэгом _span_ и классом _text_;
- автор - элемент с тэгом _small_ и классом _author_;
- тэги - элементы с тэгом _a_ и классом _tag_;

С помощью классов методами _find_(для случаев, когда элемент один) и _.find_all_(для случаев, когда элементов несколько) находим соответствующие элементы, методом _.text_ извлекаем текст из элемента с соответствующим классом, сохраняем информацию в словарь, словарь включаем в список словарей _quotes._

Параллельно аналогичным способом ведется запись в словарь данных об авторах, а также в словарь тэгов.

В конце главной страницы записываем в словарь топ-10 тэгов.

Далее циклом проходимся по всем найденным цитатам, извлекая и сохраняя в словарь текст цитаты, автора и тэги.

Для перехода на следующие страницы и их обработки создаем цикл _while_. Ссылка перехода на следующую страницу содержится в элементе с тэгом _li_ и классом _next_. Если мы находим такой элемент на странице - значит есть следующая страница - делаем запрос к этой странице, получаем ответ, обрабатываем через BeautifulSoup и записываем данные в словарь, затем добавляем словарь к списку словарей _quotes._

После обработки всех данных они записываются в JSON-файл.

## d. Почему был выбран тот или иной метод/инструмент, а не другой

- наряду с библиотекой _BeautifulSoup_ для парсинга могут быть использованы также библиотеки _Scrapy_, _Selenium_, _lxml_, _pyquery_. Применил _BeautifulSoup_, поскольку у него простой интерфейс и поиск, для задачи парсинга представленного сайта его функционала достаточно;
- цикл перелистывания веб-страниц для считывания цитат с всего сайта можно было реализовать через _for i in range(1, 11)_, вставляя значение i в url-адрес: https://quotes.toscrape.com/page/**i**/, где **i** - номер страницы. Всего на этом сайте их 10. Однако был выбран способ while, т.к. он более общий и универсальный и позволяет считать любое количество страниц (а не только ровно 10), пока на веб-странице появляется кнопка перехода на следующую страницу;
- поиск в тексте веб-страницы после обработки _BeautifulSoup_ (переменная _page_) можно производить различными способами - по тэгам, по классам, по атрибутам и т.д., в т.ч. одновременно, например, по тэгам и их классам. Однако на анализируемом сайте все интересующие нас элементы имеют уникальный класс, поэтому поиск осуществляется только по классу;
- в JSON-файл информацию можно сохранять с перекодировкой не-ASCII символов и без перекодировки. Плюс перекодировки - более надежный машиночитаемый формат в JSON-файле. Минус - при чтении человеком в глаза бросаются "артефакты" (так, например, _André Gide_ становится _Andr\u00e9 Gide_). Для основного варианта решения задачи выбрал способ сохранения с перекодировкой как более удобный для машинной обработки и потенциально вызывающий меньше ошибок. Но в расширенном скрипте сделал также варианты JSON-файлов без перекодировки в формате, более приятном глазу.

## Примечание

- практически к каждой строчке кода в скрипте приписано примечание, поясняющее совершаемые операции;
- вложены также скрипты без комментариев - файлы с припиской \__without comm_
